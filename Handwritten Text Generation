import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding
import random

# Load and preprocess data
def load_data(file_path):
    """Load a text file and prepare character-level data."""
    with open(file_path, 'r') as file:
        text = file.read().lower()
    print(f"Corpus length: {len(text)} characters")

    # Create character mappings
    chars = sorted(list(set(text)))
    char_to_idx = {char: idx for idx, char in enumerate(chars)}
    idx_to_char = {idx: char for idx, char in enumerate(chars)}
    
    return text, chars, char_to_idx, idx_to_char

# Create training sequences
def create_sequences(text, char_to_idx, maxlen=40, step=3):
    """Create input-output sequences for training."""
    sentences = []
    next_chars = []
    
    for i in range(0, len(text) - maxlen, step):
        sentences.append(text[i: i + maxlen])
        next_chars.append(text[i + maxlen])
    
    print(f"Number of sequences: {len(sentences)}")
    
    # Vectorize the sequences
    X = np.zeros((len(sentences), maxlen, len(char_to_idx)), dtype=np.bool)
    y = np.zeros((len(sentences), len(char_to_idx)), dtype=np.bool)
    
    for i, sentence in enumerate(sentences):
        for t, char in enumerate(sentence):
            X[i, t, char_to_idx[char]] = 1
        y[i, char_to_idx[next_chars[i]]] = 1
    
    return X, y

# Build the model
def build_model(input_shape, output_dim):
    """Build an LSTM-based character-level model."""
    model = Sequential()
    model.add(LSTM(128, input_shape=input_shape))
    model.add(Dense(output_dim, activation='softmax'))
    
    model.compile(optimizer='adam', loss='categorical_crossentropy')
    return model

# Generate text
def generate_text(model, seed_text, char_to_idx, idx_to_char, length=200, temperature=1.0):
    """Generate text given a seed string."""
    generated = ''
    sentence = seed_text.lower()
    
    for _ in range(length):
        x_pred = np.zeros((1, len(sentence), len(char_to_idx)))
        for t, char in enumerate(sentence):
            if char in char_to_idx:
                x_pred[0, t, char_to_idx[char]] = 1
        
        preds = model.predict(x_pred, verbose=0)[0]
        preds = np.asarray(preds).astype('float64')
        preds = np.log(preds + 1e-10) / temperature  # Adjust temperature
        exp_preds = np.exp(preds)
        preds = exp_preds / np.sum(exp_preds)
        
        next_idx = np.random.choice(len(preds), p=preds)
        next_char = idx_to_char[next_idx]
        
        generated += next_char
        sentence = sentence[1:] + next_char
    
    return generated

# Main execution
if _name_ == '_main_':
    # Load data
    file_path = 'handwritten_text.txt'  # Replace with your dataset path
    text, chars, char_to_idx, idx_to_char = load_data(file_path)
    
    # Prepare sequences
    maxlen = 40
    step = 3
    X, y = create_sequences(text, char_to_idx, maxlen, step)
    
    # Build model
    model = build_model(input_shape=(maxlen, len(chars)), output_dim=len(chars))
    model.summary()
    
    # Train model
    model.fit(X, y, batch_size=128, epochs=10)
    
    # Generate text
    seed_text = text[:40]
    print("Generating text:")
    print(generate_text(model, seed_text, char_to_idx, idx_to_char, length=400, temperature=0.5))
