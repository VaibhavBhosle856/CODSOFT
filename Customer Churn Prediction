Develop a model to predict customer churn for a subscription based service or business. Use historical and customer demographics, and try algorithms like Logistic Regression, Random Forests, or Gradient Boosting to predict chur
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# Step 1: Load the dataset
def load_data(file_path):
    """Load customer churn dataset."""
    data = pd.read_csv(file_path)
    print("Dataset Head:\n", data.head())
    print("\nDataset Info:")
    print(data.info())
    return data

# Step 2: Preprocess the dataset
def preprocess_data(data, target_column):
    """Clean and preprocess data."""
    # Drop rows with missing target
    data = data.dropna(subset=[target_column])
    
    # Handle missing values (imputation or drop)
    data = data.fillna(data.median(numeric_only=True))
    
    # Encode categorical variables (one-hot encoding)
    data = pd.get_dummies(data, drop_first=True)
    
    # Separate features and target
    X = data.drop(columns=[target_column])
    y = data[target_column]
    return X, y

# Step 3: Feature scaling
def scale_features(X_train, X_test):
    """Standardize numerical features."""
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    return X_train_scaled, X_test_scaled

# Step 4: Train models
def train_models(X_train, y_train):
    """Train Logistic Regression, Random Forest, and Gradient Boosting models."""
    models = {
        'Logistic Regression': LogisticRegression(),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
    }
    
    for model_name, model in models.items():
        print(f"Training {model_name}...")
        model.fit(X_train, y_train)
        models[model_name] = model
    return models

# Step 5: Evaluate models
def evaluate_models(models, X_test, y_test):
    """Evaluate models on test data."""
    for model_name, model in models.items():
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
        
        print(f"\n--- {model_name} ---")
        print("Accuracy:", accuracy_score(y_test, y_pred))
        print("Classification Report:\n", classification_report(y_test, y_pred))
        if y_prob is not None:
            print("AUC-ROC Score:", roc_auc_score(y_test, y_prob))

# Main function
if _name_ == '_main_':
    # Load dataset
    file_path = 'customer_churn.csv'  # Replace with your dataset file path
    data = load_data(file_path)
    
    # Preprocess data
    target_column = 'Churn'  # Replace with actual target column
    X, y = preprocess_data(data, target_column)
    
    # Split data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Feature scaling
    X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
    
    # Train models
    models = train_models(X_train_scaled, y_train)
    
    # Evaluate models
    evaluate_models(models, X_test_scaled, y_test)
    
    # Test on new customer data
    new_data = np.array([[35, 120, 2, 100.0, 1]])  # Replace with appropriate feature values
    print("\nPredictions on new customer data:")
    for model_name, model in models.items():
        prediction = model.predict(new_data)
        print(f"{model_name} Prediction:", "Churn" if prediction[0] == 1 else "No Churn")
